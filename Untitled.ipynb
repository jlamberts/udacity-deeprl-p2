{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "female-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worse-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "western-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "# deep rl in continuous action spaces https://arxiv.org/pdf/1509.02971.pdf\n",
    "# https://github.com/ShangtongZhang/DeepRL/blob/master/deep_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liberal-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityEnvWrapper:\n",
    "    \"\"\"Little helper class to initialize a unity env and make it a little easier to interact with.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, file_name=\"Reacher_Linux_NoVis/Reacher.x86_64\", train_mode=True, **kwargs\n",
    "    ):\n",
    "        self.env = UnityEnvironment(file_name=file_name, **kwargs)\n",
    "        self._env_info = None\n",
    "        self.train_mode = train_mode\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        self.num_agents = len(self.env_info.agents)\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        self.state_size = self.states.shape[1]\n",
    "\n",
    "    @property\n",
    "    def env_info(self):\n",
    "        if not self._env_info:\n",
    "            self._env_info = self.env.reset(train_mode=self.train_mode)[self.brain_name]\n",
    "        return self._env_info\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        return self.env_info.vector_observations\n",
    "\n",
    "    @property\n",
    "    def step_tuple(self):\n",
    "        \"\"\"(next_state, reward, done)\"\"\"\n",
    "        return (\n",
    "            torch.tensor(self.env_info.vector_observations, dtype=torch.float),\n",
    "            torch.tensor(self.env_info.rewards, dtype=torch.float).unsqueeze(-1),\n",
    "            self.env_info.local_done,\n",
    "        )\n",
    "\n",
    "    def get_random_actions(self, n_agents=None, clip=True):\n",
    "        \"\"\"Get random actions for `n_agents` agents, sampled from the random normal distribution.\n",
    "\n",
    "        If n_agents is not provided, one random action will be generated per agent in the environment.\n",
    "        If `clip` is set to True, values will be clipped between [-1,1]\n",
    "        \"\"\"\n",
    "        if not n_agents:\n",
    "            n_agents = self.num_agents\n",
    "        unclipped = np.random.randn(n_agents, self.action_size)\n",
    "        return np.clip(unclipped, -1, 1) if clip else unclipped\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"(next_state, reward, done)\"\"\"\n",
    "        self._env_info = self.env.step(actions)[self.brain_name]\n",
    "        return self.step_tuple\n",
    "\n",
    "    def reset(self, train_mode=True):\n",
    "        self._env_info = self.env.reset(train_mode=train_mode)[self.brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improving-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civil-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liquid-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"Actor Critic network for continuous action spaces.  This network assumes that the output\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size=33,\n",
    "        action_size=4,\n",
    "        hidden_layer_size=256,\n",
    "        seed=42,\n",
    "        batchnorm_inputs=True,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            hidden_layer_size (int): Size of the hidden layer\n",
    "            seed (int): Random seed\n",
    "            batchnorm_inputs (bool): if True, apply batch normalization to the inputs\n",
    "                Per Lillicrap et al (2016) this can help training and generalization with different physical dimensions for inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.batchnorm_layer = nn.BatchNorm1d(state_size) if batchnorm_inputs else None\n",
    "\n",
    "        # Using separate networks: https://datascience.stackexchange.com/questions/35814/confusion-about-neural-network-architecture-for-the-actor-critic-reinforcement-l\n",
    "        self.inputs_critic = nn.Linear(state_size, hidden_layer_size)\n",
    "        self.outputs_critic = nn.Linear(hidden_layer_size, 1)\n",
    "\n",
    "        self.inputs_actor = nn.Linear(state_size, hidden_layer_size)\n",
    "        self.action_means = nn.Linear(hidden_layer_size, action_size)\n",
    "        self.action_stds = nn.Linear(hidden_layer_size, action_size)\n",
    "\n",
    "    #         self.std = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> policy, value\"\"\"\n",
    "        if self.batchnorm_layer:\n",
    "            state = self.batchnorm_layer(state)\n",
    "\n",
    "        # calculate policy using actor network\n",
    "        value = self.inputs_critic(state)\n",
    "        value = F.relu(value)\n",
    "        value = self.outputs_critic(value)\n",
    "        # no need for a non-linear activation here\n",
    "\n",
    "        # calculate policy using actor network\n",
    "        policy = self.inputs_actor(state)\n",
    "        policy = F.relu(policy)\n",
    "\n",
    "        # tanh will give us an output in the range (-1, 1)\n",
    "        policy_mean = F.tanh(self.action_means(policy))\n",
    "\n",
    "        # std needs to be positive, so we can use softmax for that\n",
    "        policy_std = self.action_stds(policy)\n",
    "        # torch.clamp(policy_std.exp(), 1e-3, 50)\n",
    "        # create one distribution per action\n",
    "        policy_dist = torch.distributions.Normal(policy_mean, policy_std)\n",
    "\n",
    "        actions = torch.clamp(policy_dist.sample(), -1, 1)\n",
    "        #         actions = policy_dist.sample()\n",
    "        return {\n",
    "            \"value\": value,\n",
    "            \"actions\": actions,\n",
    "            \"log_probs\": policy_dist.log_prob(actions).sum(-1, keepdim=True),\n",
    "            \"entropy\": policy_dist.entropy().sum(-1, keepdim=True),\n",
    "            \"mean\": policy_mean,\n",
    "            \"std\": policy_std,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "breeding-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor Critic network for continuous action spaces.  This network assumes that the output\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size=33,\n",
    "        action_size=4,\n",
    "        hidden_layer_size=256,\n",
    "        seed=42,\n",
    "        batchnorm_inputs=True,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            hidden_layer_size (int): Size of the hidden layer\n",
    "            seed (int): Random seed\n",
    "            batchnorm_inputs (bool): if True, apply batch normalization to the inputs\n",
    "                Per Lillicrap et al (2016) this can help training and generalization with different physical dimensions for inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.batchnorm_layer = nn.BatchNorm1d(state_size) if batchnorm_inputs else None\n",
    "\n",
    "        self.inputs_actor = nn.Linear(state_size, hidden_layer_size)\n",
    "        self.action_means = nn.Linear(hidden_layer_size, action_size)\n",
    "        self.action_stds = nn.Linear(hidden_layer_size, action_size)\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> policy, value\"\"\"\n",
    "        if self.batchnorm_layer:\n",
    "            state = self.batchnorm_layer(state)\n",
    "\n",
    "        # calculate policy using actor network\n",
    "        policy = self.inputs_actor(state)\n",
    "        policy = F.relu(policy)\n",
    "\n",
    "        # tanh will give us an output in the range (-1, 1)\n",
    "        policy_mean = F.tanh(self.action_means(policy))\n",
    "\n",
    "        # std needs to be positive, so we can use softmax for that\n",
    "        #         policy_log_std = self.action_stds(policy)\n",
    "\n",
    "        # create one distribution per action\n",
    "        policy_dist = torch.distributions.Normal(\n",
    "            policy_mean, torch.clamp(self.log_std.exp(), 1e-3, 50)\n",
    "        )\n",
    "\n",
    "        return policy_dist\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Actor Critic network for continuous action spaces.  This network assumes that the output\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size=33,\n",
    "        action_size=4,\n",
    "        hidden_layer_size=256,\n",
    "        seed=42,\n",
    "        batchnorm_inputs=True,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            hidden_layer_size (int): Size of the hidden layer\n",
    "            seed (int): Random seed\n",
    "            batchnorm_inputs (bool): if True, apply batch normalization to the inputs\n",
    "                Per Lillicrap et al (2016) this can help training and generalization with different physical dimensions for inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.batchnorm_layer = nn.BatchNorm1d(state_size) if batchnorm_inputs else None\n",
    "\n",
    "        # Using separate networks: https://datascience.stackexchange.com/questions/35814/confusion-about-neural-network-architecture-for-the-actor-critic-reinforcement-l\n",
    "        self.inputs_critic = nn.Linear(state_size, hidden_layer_size)\n",
    "        self.outputs_critic = nn.Linear(hidden_layer_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> policy, value\"\"\"\n",
    "        if self.batchnorm_layer:\n",
    "            state = self.batchnorm_layer(state)\n",
    "        # calculate policy using actor network\n",
    "        value = self.inputs_critic(state)\n",
    "        value = F.relu(value)\n",
    "        value = self.outputs_critic(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "helpful-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self):\n",
    "    config = self.config\n",
    "    storage = Storage(config.rollout_length)\n",
    "    states = self.states\n",
    "    for _ in range(config.rollout_length):\n",
    "        prediction = self.network(config.state_normalizer(states))\n",
    "        next_states, rewards, terminals, info = self.task.step(\n",
    "            to_np(prediction[\"action\"])\n",
    "        )\n",
    "        self.record_online_return(info)\n",
    "        rewards = config.reward_normalizer(rewards)\n",
    "        storage.feed(prediction)\n",
    "        storage.feed(\n",
    "            {\n",
    "                \"reward\": tensor(rewards).unsqueeze(-1),\n",
    "                \"mask\": tensor(1 - terminals).unsqueeze(-1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        states = next_states\n",
    "        self.total_steps += config.num_workers\n",
    "\n",
    "    # get current prediction\n",
    "    self.states = states\n",
    "    prediction = self.network(config.state_normalizer(states))\n",
    "    storage.feed(prediction)\n",
    "    storage.placeholder()\n",
    "    advantages = tensor(np.zeros((config.num_workers, 1)))\n",
    "    returns = prediction[\"v\"].detach()\n",
    "    for i in reversed(range(config.rollout_length)):\n",
    "        # set returns to discounted sum of future rewards\n",
    "        returns = storage.reward[i] + config.discount * storage.mask[i] * returns\n",
    "        if not config.use_gae:\n",
    "            advantages = returns - storage.v[i].detach()\n",
    "        else:\n",
    "            td_error = (\n",
    "                storage.reward[i]\n",
    "                + config.discount * storage.mask[i] * storage.v[i + 1]\n",
    "                - storage.v[i]\n",
    "            )\n",
    "            advantages = (\n",
    "                advantages * config.gae_tau * config.discount * storage.mask[i]\n",
    "                + td_error\n",
    "            )\n",
    "        storage.advantage[i] = advantages.detach()\n",
    "        storage.ret[i] = returns.detach()\n",
    "\n",
    "    # concatenate all into a single batch\n",
    "    entries = storage.extract([\"log_pi_a\", \"v\", \"ret\", \"advantage\", \"entropy\"])\n",
    "    policy_loss = -(entries.log_pi_a * entries.advantage).mean()\n",
    "    value_loss = 0.5 * (entries.ret - entries.v).pow(2).mean()\n",
    "    entropy_loss = entries.entropy.mean()\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    (\n",
    "        policy_loss\n",
    "        - config.entropy_weight * entropy_loss\n",
    "        + config.value_loss_weight * value_loss\n",
    "    ).backward()\n",
    "    nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip)\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "joint-brief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 finished with average score 5.173999707796611\n",
      "actor loss 3.5673303646908607\n",
      "critic loss 12.465995586593635\n",
      "average mean tensor([-0.4633, -2.9811,  2.4961, -3.0023])\n",
      "average std tensor([ 4.6201,  4.7476,  4.6436,  4.8190])\n",
      "episode 1 finished with average score 8.043999593472108\n",
      "actor loss -0.9731268243922386\n",
      "critic loss 3.1064078664639965\n",
      "average mean tensor([ 1.8209, -2.4787,  1.5606, -3.3080])\n",
      "average std tensor([ 4.6352,  4.7000,  4.3361,  4.6268])\n",
      "episode 2 finished with average score 12.462999418494292\n",
      "actor loss -0.6630310152831953\n",
      "critic loss 2.343846908363048\n",
      "average mean tensor([ 1.7030, -3.5597,  1.4252, -2.6987])\n",
      "average std tensor([ 4.6676,  4.6758,  4.2125,  4.4427])\n",
      "episode 3 finished with average score 11.741999465972185\n",
      "actor loss -0.8908588718040846\n",
      "critic loss 1.7199748730636202\n",
      "average mean tensor([ 1.1371, -3.0342,  1.3721, -2.4356])\n",
      "average std tensor([ 4.7959,  4.6363,  4.1748,  4.2560])\n",
      "episode 4 finished with average score 15.37649934773799\n",
      "actor loss -0.7275919401145075\n",
      "critic loss 1.8625972098088823\n",
      "average mean tensor([ 1.6312, -3.2078,  0.8404, -2.6540])\n",
      "average std tensor([ 4.7696,  4.5681,  4.0900,  4.2334])\n",
      "episode 5 finished with average score 17.809499273542315\n",
      "actor loss -0.6516902297735214\n",
      "critic loss 1.72196003125282\n",
      "average mean tensor([ 2.4753, -2.1139,  0.9995, -2.0600])\n",
      "average std tensor([ 4.7261,  4.4687,  4.0253,  4.4321])\n",
      "episode 6 finished with average score 18.62149932666216\n",
      "actor loss -1.0864151053247042\n",
      "critic loss 1.580249167920556\n",
      "average mean tensor([ 1.7822, -0.8590,  1.1178, -1.6889])\n",
      "average std tensor([ 4.5160,  4.4106,  3.8688,  4.5816])\n",
      "episode 7 finished with average score 26.03299907571636\n",
      "actor loss -0.9944568585488014\n",
      "critic loss 2.1724312144797295\n",
      "average mean tensor([ 0.8090, -0.5823,  0.8394, -1.8437])\n",
      "average std tensor([ 4.2343,  4.1993,  3.6092,  4.5148])\n",
      "episode 8 finished with average score 35.89599889901001\n",
      "actor loss -0.5385352631856222\n",
      "critic loss 2.939110619481653\n",
      "average mean tensor([ 0.1076, -0.0341,  0.4984, -1.6531])\n",
      "average std tensor([ 4.1431,  4.1252,  3.4529,  4.5426])\n",
      "episode 9 finished with average score 38.45499879214913\n",
      "actor loss -2.5444798643584363\n",
      "critic loss 4.056202540174127\n",
      "average mean tensor([ 0.1858, -0.2566, -0.2625, -0.5399])\n",
      "average std tensor([ 3.9820,  4.0469,  3.4560,  4.4129])\n",
      "episode 10 finished with average score 50.89349850942381\n",
      "actor loss -1.723244413093198\n",
      "critic loss 4.424959827156272\n",
      "average mean tensor([-0.1566, -0.2635,  0.9628, -1.3423])\n",
      "average std tensor([ 3.8750,  3.9710,  3.6387,  4.4486])\n",
      "episode 11 finished with average score 53.10799849173054\n",
      "actor loss -2.7302528386353515\n",
      "critic loss 4.662034960347228\n",
      "average mean tensor([-0.4900, -0.0884,  0.7642, -0.5097])\n",
      "average std tensor([ 3.8683,  3.9413,  3.5780,  4.5184])\n",
      "episode 12 finished with average score 67.56449788925238\n",
      "actor loss -3.41313557542162\n",
      "critic loss 6.137323471310083\n",
      "average mean tensor([-0.9573, -0.1963,  0.8707, -0.6959])\n",
      "average std tensor([ 3.7487,  3.9312,  3.5064,  4.5057])\n",
      "episode 13 finished with average score 67.32249799941201\n",
      "actor loss -2.864119752746774\n",
      "critic loss 6.176175150554627\n",
      "average mean tensor([-0.1756,  0.2541,  0.7401, -0.3460])\n",
      "average std tensor([ 3.6065,  3.9817,  3.4613,  4.5600])\n",
      "episode 14 finished with average score 65.34399809478782\n",
      "actor loss -3.7900537865643855\n",
      "critic loss 7.911897019250318\n",
      "average mean tensor([-0.8738, -0.0987,  0.8441, -0.7703])\n",
      "average std tensor([ 3.5373,  3.9833,  3.5084,  4.5517])\n",
      "episode 15 finished with average score 62.316498126485385\n",
      "actor loss -4.781705949164461\n",
      "critic loss 5.847258324851282\n",
      "average mean tensor([-0.4905, -0.1675,  1.0210, -0.9760])\n",
      "average std tensor([ 3.5084,  3.9607,  3.5343,  4.4713])\n",
      "episode 16 finished with average score 70.52199780114461\n",
      "actor loss -2.687311222689459\n",
      "critic loss 6.6152066183276474\n",
      "average mean tensor([-0.8471,  0.1542,  0.8573, -1.0218])\n",
      "average std tensor([ 3.5261,  4.0204,  3.4925,  4.4367])\n",
      "episode 17 finished with average score 81.48899776861072\n",
      "actor loss -3.4494926413462963\n",
      "critic loss 7.939987814752385\n",
      "average mean tensor([-1.0595, -0.3632,  0.3469, -0.1415])\n",
      "average std tensor([ 3.6408,  4.1681,  3.4834,  4.3269])\n",
      "episode 18 finished with average score 88.98399755719583\n",
      "actor loss -5.141279985778965\n",
      "critic loss 9.286471787374467\n",
      "average mean tensor([-1.0716, -0.3256,  0.0553,  0.0746])\n",
      "average std tensor([ 3.6089,  4.2802,  3.4510,  4.2097])\n",
      "episode 19 finished with average score 93.59499749506358\n",
      "actor loss -5.789672653278103\n",
      "critic loss 9.642100758384913\n",
      "average mean tensor([-1.0656, -0.4874,  0.1399,  0.1376])\n",
      "average std tensor([ 3.5027,  4.2867,  3.4141,  4.1418])\n",
      "episode 20 finished with average score 97.33699740690645\n",
      "actor loss -5.613220351428026\n",
      "critic loss 10.652726394124329\n",
      "average mean tensor([-0.9368, -0.3681,  0.2627, -0.3081])\n",
      "average std tensor([ 3.4511,  4.3290,  3.3628,  4.0106])\n",
      "episode 21 finished with average score 92.82899745099712\n",
      "actor loss -5.807617958082119\n",
      "critic loss 9.625207635341212\n",
      "average mean tensor([-0.8377, -0.3466,  0.1410, -0.3538])\n",
      "average std tensor([ 3.4883,  4.3835,  3.3467,  3.9348])\n",
      "episode 22 finished with average score 85.52199776493944\n",
      "actor loss -5.879722533194581\n",
      "critic loss 9.120609818026423\n",
      "average mean tensor([-0.8213, -0.3653,  0.2203, -0.3130])\n",
      "average std tensor([ 3.4508,  4.4602,  3.3344,  3.8816])\n",
      "episode 23 finished with average score 90.94549763249233\n",
      "actor loss -4.8222141644509975\n",
      "critic loss 10.42999688279815\n",
      "average mean tensor([-0.0469,  0.2086,  0.1490, -0.6708])\n",
      "average std tensor([ 3.4492,  4.4298,  3.3059,  3.8116])\n",
      "episode 24 finished with average score 78.63199757249095\n",
      "actor loss -4.935297158837784\n",
      "critic loss 7.777266270248219\n",
      "average mean tensor([ 0.7017,  0.9479,  0.1511, -0.2110])\n",
      "average std tensor([ 3.4405,  4.4159,  3.2818,  3.7385])\n",
      "episode 25 finished with average score 79.76249765686225\n",
      "actor loss -4.683942289382685\n",
      "critic loss 7.9007102872710675\n",
      "average mean tensor([ 0.5880, -0.0995,  0.1070, -0.4486])\n",
      "average std tensor([ 3.5001,  4.4573,  3.3556,  3.7661])\n",
      "episode 26 finished with average score 107.82549715018831\n",
      "actor loss -3.4721986863587517\n",
      "critic loss 11.418279510573484\n",
      "average mean tensor([ 0.2767,  0.1957, -0.1867, -0.3726])\n",
      "average std tensor([ 3.4525,  4.4277,  3.3297,  3.6676])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e5ca0bc2757b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mpolicy_means\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mpolicy_stds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             episode_done_mask = (1.0 - torch.tensor(done, dtype=torch.float)).unsqueeze(\n",
      "\u001b[0;32m<ipython-input-4-f2901666f50f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;34m\"\"\"(next_state, reward, done)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "actor = ActorNetwork(hidden_layer_size=64, batchnorm_inputs=False)\n",
    "critic = CriticNetwork(hidden_layer_size=64, batchnorm_inputs=False)\n",
    "\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=1e-4)\n",
    "\n",
    "scores = []\n",
    "num_episodes = 100\n",
    "gamma = 0.99\n",
    "entropy_weight = 5e-4\n",
    "rollout_length = 5\n",
    "\n",
    "\n",
    "def get_tensor_from_rollout(rollout, key):\n",
    "    return torch.cat([r[key] for r in rollout])\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    env.reset()\n",
    "    state = torch.tensor(env.states, dtype=torch.float)\n",
    "    step_count = 0\n",
    "    actor_loss_total = 0\n",
    "    critic_loss_total = 0\n",
    "    policy_means = torch.zeros(4)\n",
    "    policy_stds = torch.zeros(4)\n",
    "\n",
    "    while not done:\n",
    "        rollout = []\n",
    "        for _ in range(rollout_length):\n",
    "            values = critic(state)\n",
    "            dists = actor(state)\n",
    "            actions = torch.clamp(dists.sample(), -1, 1)\n",
    "            policy_means += dists.mean.mean(0)\n",
    "            policy_stds += dists.stddev.mean(0)\n",
    "            next_state, reward, done = env.step(actions.numpy())\n",
    "            score += float(reward.mean())\n",
    "            episode_done_mask = (1.0 - torch.tensor(done, dtype=torch.float)).unsqueeze(\n",
    "                -1\n",
    "            )\n",
    "\n",
    "            rollout.append(\n",
    "                {\n",
    "                    \"state\": state,\n",
    "                    \"value\": values,\n",
    "                    \"actions\": actions,\n",
    "                    \"reward\": reward,\n",
    "                    \"done_mask\": episode_done_mask,\n",
    "                    \"log_probs\": dists.log_prob(actions),\n",
    "                    \"entropy\": dists.entropy(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        future_value = critic(state)\n",
    "        for i in reversed(range(rollout_length)):\n",
    "            rollout_dict = rollout[i]\n",
    "\n",
    "            future_value = (\n",
    "                rollout_dict[\"reward\"]\n",
    "                + gamma * future_value * rollout_dict[\"done_mask\"]\n",
    "            )\n",
    "            advantage = future_value - rollout_dict[\"value\"]\n",
    "            rollout_dict[\"future_value\"] = future_value.detach()\n",
    "            rollout_dict[\"advantage\"] = advantage.detach()\n",
    "\n",
    "        advantage = get_tensor_from_rollout(rollout, \"advantage\")\n",
    "        log_probs = get_tensor_from_rollout(rollout, \"log_probs\")\n",
    "        entropy = get_tensor_from_rollout(rollout, \"entropy\")\n",
    "        future_value = get_tensor_from_rollout(rollout, \"future_value\")\n",
    "        value = get_tensor_from_rollout(rollout, \"value\")\n",
    "\n",
    "        critic_loss = 0.5 * (future_value - value).pow(2).mean()\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(critic.parameters(), 1)\n",
    "        critic_optimizer.step()\n",
    "        critic_loss_total += float(critic_loss)\n",
    "\n",
    "        policy_loss = -(log_probs * advantage.detach()).mean()\n",
    "        entropy_loss = -entropy.mean() * entropy_weight\n",
    "        actor_loss = policy_loss + entropy_loss\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(actor.parameters(), 1)\n",
    "        actor_optimizer.step()\n",
    "        actor_loss_total += float(actor_loss)\n",
    "\n",
    "        #         torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "\n",
    "        done = any(done)\n",
    "        step_count += 1\n",
    "    if episode % 1 == 0:\n",
    "        print(f\"episode {episode} finished with average score {score}\")\n",
    "        print(\"actor loss\", actor_loss_total)\n",
    "        print(\"critic loss\", critic_loss_total)\n",
    "        print(\"average mean\", policy_means / 1000)\n",
    "        print(\"average std\", policy_stds / 1000)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "raising-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorNetwork(\n",
       "  (inputs_actor): Linear(in_features=33, out_features=64, bias=True)\n",
       "  (action_means): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (action_stds): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(actor.state_dict(), \"actor_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "experimental-intermediate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriticNetwork(\n",
       "  (inputs_critic): Linear(in_features=33, out_features=64, bias=True)\n",
       "  (outputs_critic): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.tensor(env.states, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = actor(states)\n",
    "actions = torch.clamp(dists.sample(), -1, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done = env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "floppy-tender",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-78ab0d70655b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f2901666f50f>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, train_mode)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, train_mode, config, lesson)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 261\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_reset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             )\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/udacity-deeprl/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "next_state = torch.tensor(env.states, dtype=torch.float)\n",
    "all_rewards = torch.zeros((20, 1))\n",
    "done = [False]\n",
    "while not any(done):\n",
    "    dists = actor(next_state)\n",
    "    actions = torch.clamp(dists.sample(), -1, 1).numpy()\n",
    "    next_state, reward, done = env.step(actions)\n",
    "    all_rewards += reward\n",
    "all_rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-partner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([r[\"reward\"] for r in rollout]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout[4][\"reward\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorNetwork(hidden_layer_size=64, batchnorm_inputs=False)\n",
    "critic = CriticNetwork(hidden_layer_size=64, batchnorm_inputs=False)\n",
    "\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=1e-4)\n",
    "\n",
    "scores = []\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "entropy_weight = 1e-6\n",
    "rollout_length = 5\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    env.reset()\n",
    "    state = torch.tensor(env.states, dtype=torch.float)\n",
    "    actor_loss_total  = 0\n",
    "    critic_loss_total = 0\n",
    "    policy_means = torch.zeros(4)\n",
    "    policy_stds = torch.zeros(4)\n",
    "    \n",
    "    while not done:\n",
    "        values = critic(state)\n",
    "        dists = actor(state)\n",
    "        actions = torch.clamp(dists.sample(), -1, 1)\n",
    "        policy_means += dists.mean.mean(0)\n",
    "        policy_stds += dists.stddev.mean(0)\n",
    "        log_probs = dists.log_prob(actions)\n",
    "        entropy = dists.entropy()\n",
    "        \n",
    "        next_state, reward, done = env.step(actions.numpy())\n",
    "        reward = torch.tensor(reward, dtype=torch.float).unsqueeze(-1)\n",
    "        score += float(reward.mean())\n",
    "        next_value = critic(next_state)\n",
    "        episode_done_mask = (1.0 - torch.tensor(done, dtype=torch.float)).unsqueeze(-1)\n",
    "        advantage = (\n",
    "            reward\n",
    "            + gamma *next_value * episode_done_mask\n",
    "            - values\n",
    "        )\n",
    "        \n",
    "        critic_loss =.5*advantage.pow(2).mean()\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(critic.parameters(), 1)\n",
    "        critic_optimizer.step()\n",
    "        critic_loss_total += float(critic_loss)\n",
    "        \n",
    "        policy_loss = -(log_probs * advantage.detach()).mean()\n",
    "        entropy_loss = -entropy.mean() * entropy_weight\n",
    "        actor_loss = policy_loss + entropy_loss\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(actor.parameters(), 1)\n",
    "        actor_optimizer.step()\n",
    "        actor_loss_total += float(actor_loss)\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "\n",
    "        state = next_state\n",
    "        done = any(done)\n",
    "    if episode % 2 == 0:\n",
    "        print(f\"episode {episode} finished with average score {score}\")\n",
    "        print(\"actor loss\", actor_loss_total)\n",
    "        print(\"critic loss\", critic_loss_total)\n",
    "        print(\"average mean\", policy_means/1000)\n",
    "        print(\"average std\", policy_stds/1000)\n",
    "#         print(\"means\", predictions[\"mean\"].mean(0))\n",
    "#         print(\"stds\", predictions[\"std\"].mean(0))\n",
    "#         if episode > 10:\n",
    "#             print(f\"10-episode rolling average: {sum(scores[episode-10:episode])/10}\")\n",
    "#         print(\"policy loss:\", policy_loss)\n",
    "#         print(\"value loss:\", value_loss)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorNetwork()\n",
    "critic = CriticNetwork()\n",
    "\n",
    "actor_optimizer = torch.optim.RMSprop(actor.parameters(), lr=0.0007)\n",
    "critic_optimizer = torch.optim.RMSprop(critic.parameters(), lr=0.0007)\n",
    "\n",
    "scores = []\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "entropy_weight = 0\n",
    "rollout_length = 5\n",
    "\n",
    "env.reset()\n",
    "state = torch.tensor(env.states, dtype=torch.float)\n",
    "\n",
    "values = critic(state)\n",
    "dists = actor(state)\n",
    "\n",
    "values = critic(state)\n",
    "dists = actor(state)\n",
    "actions = dists.sample()\n",
    "log_probs = dists.log_prob(actions)\n",
    "entropy = dists.entropy()\n",
    "\n",
    "next_state, reward, done = env.step(actions.numpy())\n",
    "\n",
    "next_value = critic(next_state)\n",
    "\n",
    "advantage = (\n",
    "    reward\n",
    "    + gamma * next_value * (1.0 - torch.tensor(done, dtype=torch.float)).unsqueeze(-1)\n",
    "    - values\n",
    ")\n",
    "\n",
    "advantage\n",
    "\n",
    "episode_done_mask = (1.0 - torch.tensor(done, dtype=torch.float)).unsqueeze(-1)\n",
    "\n",
    "entropy_weight = 1e-5\n",
    "-entropy.mean() * entropy_weight\n",
    "\n",
    "actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "\n",
    "actor_loss\n",
    "\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ActorCriticNetwork(batchnorm_inputs=True)\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "entropy_weight = 1e-5\n",
    "rollout_length = 5\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    env.reset()\n",
    "    state = torch.tensor(env.states, dtype=torch.float)\n",
    "    step_count = 0\n",
    "    while not done:\n",
    "        predictions = net(torch.tensor(state, dtype=torch.float))\n",
    "        values = predictions[\"value\"]\n",
    "        actions = predictions[\"actions\"]\n",
    "        log_probs = predictions[\"log_probs\"]\n",
    "        next_state, reward, done = env.step(actions.numpy())\n",
    "        reward = torch.tensor(reward, dtype=torch.float).unsqueeze(-1)\n",
    "        score += reward.sum()\n",
    "#         with torch.no_grad():\n",
    "        next_value =  net(torch.tensor(next_state, dtype=torch.float))[\"value\"]\n",
    "#         with torch.no_grad():\n",
    "        advantage = (\n",
    "            reward\n",
    "            + gamma *next_value * (1.0 - torch.tensor(done, dtype=torch.float)).unsqueeze(-1)\n",
    "            - values\n",
    "        )\n",
    "\n",
    "        value_loss =.5*advantage.pow(2).mean()\n",
    "        policy_loss = -(log_probs * advantage.detach()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        (policy_loss + value_loss - entropy_weight * predictions[\"entropy\"].mean()).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        state = next_state\n",
    "        done = any(done)\n",
    "        step_count += 1\n",
    "    score = score / 20\n",
    "    if episode % 2 == 0:\n",
    "        print(f\"episode {episode} finished with average score {score}\")\n",
    "        print(\"means\", predictions[\"mean\"].mean(0))\n",
    "        print(\"stds\", predictions[\"std\"].mean(0))\n",
    "#         if episode > 10:\n",
    "#             print(f\"10-episode rolling average: {sum(scores[episode-10:episode])/10}\")\n",
    "#         print(\"policy loss:\", policy_loss)\n",
    "#         print(\"value loss:\", value_loss)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollout_length = 5\n",
    "# storage = []\n",
    "# for _ in range(rollout_length):\n",
    "#     prediction = net(torch.tensor(env.states, dtype=torch.float))\n",
    "#     next_states, rewards, dones = env.step(prediction[\"actions\"].numpy())\n",
    "#     storage.append((env.states, prediction, rewards, next_states, dones))\n",
    "\n",
    "#     states = next_states\n",
    "\n",
    "# # self.states = states\n",
    "# # prediction = self.network(config.state_normalizer(states))\n",
    "# # storage.feed(prediction)\n",
    "# # storage.placeholder()\n",
    "\n",
    "# # advantages = tensor(np.zeros((config.num_workers, 1)))\n",
    "# # returns = prediction['v'].detach()\n",
    "# # for i in reversed(range(config.rollout_length)):\n",
    "# #     returns = storage.reward[i] + config.discount * storage.mask[i] * returns\n",
    "# #     if not config.use_gae:\n",
    "# #         advantages = returns - storage.v[i].detach()\n",
    "# #     else:\n",
    "# #         td_error = storage.reward[i] + config.discount * storage.mask[i] * storage.v[i + 1] - storage.v[i]\n",
    "# #         advantages = advantages * config.gae_tau * config.discount * storage.mask[i] + td_error\n",
    "# #     storage.advantage[i] = advantages.detach()\n",
    "# #     storage.ret[i] = returns.detach()\n",
    "\n",
    "# # entries = storage.extract(['log_pi_a', 'v', 'ret', 'advantage', 'entropy'])\n",
    "# # policy_loss = -(entries.log_pi_a * entries.advantage).mean()\n",
    "# # value_loss = 0.5 * (entries.ret - entries.v).pow(2).mean()\n",
    "# # entropy_loss = entries.entropy.mean()\n",
    "\n",
    "# # self.optimizer.zero_grad()\n",
    "# # (policy_loss - config.entropy_weight * entropy_loss +\n",
    "# #  config.value_loss_weight * value_loss).backward()\n",
    "# # nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip)\n",
    "# # self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    def __init__(self, network, env):\n",
    "        self.env = env\n",
    "        self.network = network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_loss = advantage * values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss = -log_probs * advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "(policy_loss + value_loss).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "res =net(torch.tensor(env.states, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.clamp(res[\"actions\"], -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-wireless",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-sunday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
